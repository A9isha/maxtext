{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53230923-1a34-41a8-a266-6aea5a61936a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 23:22:40.493575: I external/xla/xla/pjrt/pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/mazumdera/.local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "2023-10-06 23:22:40.493608: I external/xla/xla/pjrt/pjrt_api.cc:67] PJRT_Api is set for device type tpu\n",
      "2023-10-06 23:22:40.493622: I external/xla/xla/pjrt/pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.32. The framework PJRT API version is 0.32.\n",
      "2023-10-06 23:22:43.030272: I external/xla/xla/pjrt/pjrt_c_api_client.cc:119] PjRtCApiClient created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 devices.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-06 23:22:44.767361: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-06 23:22:44.873879: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-10-06 23:22:44.874797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-06 23:22:46.312165: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/mazumdera/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Get all the imports\n",
    "import jax\n",
    "import os\n",
    "import sys\n",
    "\n",
    "jax.config.update('jax_default_prng_impl', 'unsafe_rbg')\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"0\"\n",
    "os.environ[\"LIBTPU_INIT_ARGS\"] = os.environ.get(\"LIBTPU_INIT_ARGS\",\"\") + \" --xla_tpu_spmd_rng_bit_generator_unsafe=true\"\n",
    "print(f\"Found {jax.device_count()} devices.\")\n",
    "\n",
    "from typing import Sequence\n",
    "import datetime\n",
    "from absl import app\n",
    "from flax.linen import partitioning as nn_partitioning\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from layers import Transformer\n",
    "import pyconfig\n",
    "from input_pipeline import create_data_iterator_with_tokenizer\n",
    "import max_utils\n",
    "import checkpointing\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "from jax.sharding import PartitionSpec as P\n",
    "from jax.sharding import Mesh\n",
    "\n",
    "from jax.experimental.compilation_cache import compilation_cache as cc\n",
    "\n",
    "from cloud_tpu_diagnostics import diagnostic\n",
    "from cloud_tpu_diagnostics.configuration import debug_configuration\n",
    "from cloud_tpu_diagnostics.configuration import diagnostic_configuration\n",
    "from cloud_tpu_diagnostics.configuration import stack_trace_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d448900f-ac85-403b-9056-c6c4a06cfe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#free up TPU if needed\n",
    "# !sudo lsof -w /dev/accel*\n",
    "# !pkill -9 python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5554f5e4-9055-4b56-902b-c996307daefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f70b105e-a737-4270-a37b-20a6f5cdd3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_directory=\"base_output_directory=gs://mazumdera-test-bucket/maxtext/lg/10052023/3\"\n",
    "file_pattern_for_train_data=\"file_pattern_for_train_data=gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\"\n",
    "file_pattern_for_eval_data=\"file_pattern_for_eval_data=gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f793b937-6902-4e8e-a7ae-317ede78430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commandline_args = [MaxText/configs/base.yml run_name=1xv3-32 save_period=5 dcn_data_parallelism=1 ici_data_parallelism=2 ici_tensor_parallelism=4 steps=20 enable_profiler=true remat_policy=full base_emb_dim=4096 base_num_heads=64 head_dim=64 vocab_size=50272 base_num_decoder_layers=16 per_device_batch_size=0.5 file_pattern_for_train_data=\"gs://yejingxin-us-central2/external/lg/dummy-data/train/*.tfrecords\" file_pattern_for_eval_data=\"gs://yejingxin-us-central2/external/lg/dummy-data/valid/*tfrecords\" enable_profiler=true base_output_directory=\"gs://mazumdera-test-bucket/maxtext/lg/10052023/3\" dataset_type=\"lg\" max_predict_length=512]\n",
    "commandline_args = [\"dummy\", \"configs/base.yml\",\"run_name=1xv3-32\", \"dcn_data_parallelism=1\", \"save_period=5\",\"ici_data_parallelism=2\",\"ici_tensor_parallelism=4\",\"steps=20\",\"enable_profiler=true\",\"remat_policy=full\",\"base_emb_dim=4096\",\"base_num_heads=64\",\"head_dim=64\",\"vocab_size=50272\",\"base_num_decoder_layers=16\",\"per_device_batch_size=0.5\",\"enable_profiler=true\",file_pattern_for_train_data, file_pattern_for_eval_data, base_output_directory,\"dataset_type=\\\"lg\\\"\",\"max_predict_length=512\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5e646a8-7808-4aba-bb13-ddf4cbeb62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyconfig.initialize(commandline_args)\n",
    "config = pyconfig.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b40c1744-ab83-4f02-a227-89a5ac541401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating checkpoint manager...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 00:02:57.325465: I external/xla/xla/pjrt/distributed/service.cc:119] Experimental coordination service is enabled.\n",
      "2023-10-07 00:02:57.325714: I external/xla/xla/pjrt/distributed/service.cc:149] Jax service listening on 10.128.0.67:34913\n",
      "2023-10-07 00:02:57.351657: I external/tsl/tsl/distributed_runtime/coordination/coordination_service.cc:553] /job:jax_worker/replica:0/task:0 has connected to coordination service. Incarnation: 744586964027380521\n",
      "2023-10-07 00:02:57.351793: I external/tsl/tsl/distributed_runtime/coordination/coordination_service_agent.cc:304] Coordination agent has successfully connected.\n",
      "2023-10-07 00:02:57.352055: I external/xla/xla/pjrt/distributed/client.cc:132] Connected to distributed JAX controller\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint manager created!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_manager = checkpointing.create_orbax_checkpoint_manager(\n",
    "      config.checkpoint_dir,\n",
    "      config.enable_checkpointing,\n",
    "      config.async_checkpointing,\n",
    "      config.save_period,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89e86840-70e1-43ae-b2ec-15946284deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial PRNG Keys\n",
    "init_rng, nextrng = random.split(random.PRNGKey(config.init_weights_seed), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea0e9192-f669-471d-9cb6-69e192acad93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)] (num_devices: 8)\n",
      "Decided on mesh: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1)\n",
      "   TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0)\n",
      "   TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1)]]\n",
      "\n",
      " [[TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)\n",
      "   TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0)\n",
      "   TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1)]]]\n"
     ]
    }
   ],
   "source": [
    "devices_array = max_utils.create_device_mesh(config)\n",
    "mesh = Mesh(devices_array, config.mesh_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94c29d35-655b-406b-b496-c667c9736a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(config, mesh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acd81cea-5e16-42ab-b14d-adca412b615f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = optax.adamw(\n",
    "      max_utils.create_learning_rate_schedule(config),\n",
    "      b1=config.adam_b1,\n",
    "      b2=config.adam_b2,\n",
    "      eps=config.adam_eps,\n",
    "      eps_root=config.adam_eps_root,\n",
    "      weight_decay=config.adam_weight_decay,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a0a25fb0-72f5-4d2c-9426-fe76109d9c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring state from this run's directory latest step         15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1696637039.202622  203532 gcs_resource.cc:99] Using default AdmissionQueue with limit 32\n",
      "I0000 00:00:1696637039.205933  208523 google_auth_provider.cc:179] Running on GCE, using service account 630405687483-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "state, state_mesh_annotations = max_utils.setup_initial_state(model, tx, config, init_rng, mesh, checkpoint_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a3bcf8f7-3b76-4585-b96b-a0af720cbabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import max_utils\n",
    "init_train_state_partial = functools.partial(max_utils.init_train_state, model, tx,\n",
    "                                               config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c5cbf3ad-e14a-443b-bb1e-6b206e03a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the flax.training.train_state.TrainState\n",
    "abstract_state = jax.eval_shape(init_train_state_partial, init_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3899b414-dc2d-4027-b5da-b9fc9271a6aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=ShapeDtypeStruct(shape=(), dtype=int32), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fdb79be68c0>\n",
       "    mesh = Mesh(device_ids=array([[[0, 1, 2, 3]],\n",
       "    \n",
       "           [[6, 7, 4, 5]]]), axis_names=('data', 'fsdp', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'relpos_bias': {'rel_embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32), names=('heads', 'layers', 'relpos_buckets'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'key_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'query_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 4096), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fdb7a64bd00>, update=<function chain.<locals>.update_fn at 0x7fdb7a64bf40>), opt_state=(ScaleByAdamState(count=ShapeDtypeStruct(shape=(), dtype=int32), mu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'relpos_bias': {'rel_embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32), names=('heads', 'layers', 'relpos_buckets'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'key_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'query_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 4096), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}, nu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32), names=('embed', 'layers', 'mlp'), mesh=None, rules=None)}, 'wo': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32), names=('mlp', 'layers', 'embed'), mesh=None, rules=None)}}, 'pre_self_attention_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16), dtype=float32), names=('embed', 'layers'), mesh=None, rules=None)}, 'relpos_bias': {'rel_embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32), names=('heads', 'layers', 'relpos_buckets'), mesh=None, rules=None)}, 'self_attention': {'key': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'key_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'out': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32), names=('heads', 'layers', 'kv', 'embed'), mesh=None, rules=None)}, 'query': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}, 'query_layer_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(64, 16), dtype=float32), names=('heads', 'layers'), mesh=None, rules=None)}, 'value': {'kernel': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32), names=('embed', 'layers', 'heads', 'kv'), mesh=None, rules=None)}}}, 'decoder_norm': {'scale': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(4096,), dtype=float32), names=('embed',), mesh=None, rules=None)}}, 'token_embedder': {'embedding': LogicallyPartitioned(value=ShapeDtypeStruct(shape=(50272, 4096), dtype=float32), names=('vocab', 'embed'), mesh=None, rules=None)}}), EmptyState(), ScaleByScheduleState(count=ShapeDtypeStruct(shape=(), dtype=int32))))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a4f5e9aa-052f-4790-9b23-564d5890a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_logical_annotations = nn.get_partition_spec(abstract_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dd1f785c-0046-4a15-8360-33b089dc1f6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=PartitionSpec(), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fdb79be68c0>\n",
       "    mesh = Mesh(device_ids=array([[[0, 1, 2, 3]],\n",
       "    \n",
       "           [[6, 7, 4, 5]]]), axis_names=('data', 'fsdp', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'relpos_bias': {'rel_embedding': PartitionSpec('heads', 'layers', 'relpos_buckets')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'key_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'query_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fdb7a64bd00>, update=<function chain.<locals>.update_fn at 0x7fdb7a64bf40>), opt_state=(ScaleByAdamState(count=PartitionSpec(), mu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'relpos_bias': {'rel_embedding': PartitionSpec('heads', 'layers', 'relpos_buckets')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'key_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'query_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}, nu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': PartitionSpec('embed', 'layers', 'mlp')}, 'wo': {'kernel': PartitionSpec('mlp', 'layers', 'embed')}}, 'pre_self_attention_layer_norm': {'scale': PartitionSpec('embed', 'layers')}, 'relpos_bias': {'rel_embedding': PartitionSpec('heads', 'layers', 'relpos_buckets')}, 'self_attention': {'key': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'key_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'out': {'kernel': PartitionSpec('heads', 'layers', 'kv', 'embed')}, 'query': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}, 'query_layer_norm': {'scale': PartitionSpec('heads', 'layers')}, 'value': {'kernel': PartitionSpec('embed', 'layers', 'heads', 'kv')}}}, 'decoder_norm': {'scale': PartitionSpec('embed',)}}, 'token_embedder': {'embedding': PartitionSpec('vocab', 'embed')}}), EmptyState(), ScaleByScheduleState(count=PartitionSpec())))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_logical_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "265baec3-ab63-40ad-ba8f-8b1b303871e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unboxed_abstract_state = max_utils.unbox_logicallypartioned_trainstate(abstract_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9da7f697-e0b8-402c-b262-586b2451ade8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainState(step=ShapeDtypeStruct(shape=(), dtype=int32), apply_fn=<bound method Module.apply of Transformer(\n",
       "    # attributes\n",
       "    config = <pyconfig.HyperParameters object at 0x7fdb79be68c0>\n",
       "    mesh = Mesh(device_ids=array([[[0, 1, 2, 3]],\n",
       "    \n",
       "           [[6, 7, 4, 5]]]), axis_names=('data', 'fsdp', 'tensor'))\n",
       ")>, params={'decoder': {'decoder': {'mlp': {'wi': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32)}}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(4096, 16), dtype=float32)}, 'relpos_bias': {'rel_embedding': ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'key_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'query_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(4096,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 4096), dtype=float32)}}, tx=GradientTransformationExtraArgs(init=<function chain.<locals>.init_fn at 0x7fdb7a64bd00>, update=<function chain.<locals>.update_fn at 0x7fdb7a64bf40>), opt_state=(ScaleByAdamState(count=ShapeDtypeStruct(shape=(), dtype=int32), mu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32)}}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(4096, 16), dtype=float32)}, 'relpos_bias': {'rel_embedding': ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'key_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'query_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(4096,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 4096), dtype=float32)}}, nu={'decoder': {'decoder': {'mlp': {'wi': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 8192), dtype=float32)}, 'wo': {'kernel': ShapeDtypeStruct(shape=(8192, 16, 4096), dtype=float32)}}, 'pre_self_attention_layer_norm': {'scale': ShapeDtypeStruct(shape=(4096, 16), dtype=float32)}, 'relpos_bias': {'rel_embedding': ShapeDtypeStruct(shape=(64, 16, 32), dtype=float32)}, 'self_attention': {'key': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'key_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'out': {'kernel': ShapeDtypeStruct(shape=(64, 16, 64, 4096), dtype=float32)}, 'query': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}, 'query_layer_norm': {'scale': ShapeDtypeStruct(shape=(64, 16), dtype=float32)}, 'value': {'kernel': ShapeDtypeStruct(shape=(4096, 16, 64, 64), dtype=float32)}}}, 'decoder_norm': {'scale': ShapeDtypeStruct(shape=(4096,), dtype=float32)}}, 'token_embedder': {'embedding': ShapeDtypeStruct(shape=(50272, 4096), dtype=float32)}}), EmptyState(), ScaleByScheduleState(count=ShapeDtypeStruct(shape=(), dtype=int32))))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unboxed_abstract_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e472fe4-3a93-4b46-a9b7-0776a00283f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Lingvo TF config\n",
    "\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff019cd4-7ec3-4d9d-bcfd-41764f8924c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
